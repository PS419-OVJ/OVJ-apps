# -*- coding: utf-8 -*-
"""ContentbaseWisata.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19r-5YVIep-XLeMmpFVlaeWRS7oDp_T7a
"""

from collections import defaultdict
import csv
import numpy as np
from numpy import genfromtxt
import numpy.ma as ma
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
import tabulate
pd.set_option("display.precision", 1)

def load_data():
    ''' called to load preprepared data for the lab '''
    item_train = genfromtxt('item_train.csv', delimiter=",")
    user_train = genfromtxt('user_train.csv', delimiter=",")
    y_train    = genfromtxt('y_train.csv', delimiter=",")
    with open('content_item_train_header.txt', newline='') as f:
        item_features = list(csv.reader(f))[0]
    with open('content_user_train_header.txt', newline='') as f:
        user_features = list(csv.reader(f))[0]
    item_vecs = genfromtxt('item_vecs.csv', delimiter=",")

    wisata_dict = defaultdict(dict)
    count = 0

    with open('destinasi_list.csv', newline='') as csvfile:
        reader = csv.reader(csvfile, delimiter=',', quotechar='"')
        for line in reader:
            if count == 0:
                count += 1  #skip header
                #print(line) print
            else:
                count += 1
                destinasi_id = int(line[0])
                wisata_dict[destinasi_id]["title"] = line[1]
                wisata_dict[destinasi_id]["genre"] = line[2]



    return(item_train, user_train, y_train, item_features, user_features, item_vecs, wisata_dict)

def pprint_train(x_train, features, vs, u_s, maxcount=5, user=True):
    """ Prints user_train or item_train nicely """
    if user:
        flist = [".0f", ".0f", ".1f",
                 ".1f", ".1f", ".1f", ".1f", ".1f", ".1f", ".1f", ".1f", ".1f", ".1f", ".1f", ".1f", ".1f", ".1f"]
    else:
        flist = [".0f", ".0f", ".1f",
                 ".0f", ".0f", ".0f", ".0f", ".0f", ".0f", ".0f", ".0f", ".0f", ".0f", ".0f", ".0f", ".0f", ".0f"]

    head = features[:vs]
    if vs < u_s: print("error, vector start {vs} should be greater then user start {u_s}")
    for i in range(u_s):
        head[i] = "[" + head[i] + "]"
    genres = features[vs:]
    hdr = head + genres
    disp = [split_str(hdr, 5)]
    count = 0
    for i in range(0, x_train.shape[0]):
        if count == maxcount: break
        count += 1
        disp.append([x_train[i, 0].astype(int),
                     x_train[i, 1].astype(int),
                     x_train[i, 2].astype(float),
                     *x_train[i, 3:].astype(float)
                    ])
    table = tabulate.tabulate(disp, tablefmt='html', headers="firstrow", floatfmt=flist, numalign='center')
    return table


def split_str(ifeatures, smax):
    ''' split the feature name strings to tables fit '''
    ofeatures = []
    for s in ifeatures:
        if not ' ' in s:  # skip string that already have a space
            if len(s) > smax:
                mid = int(len(s)/2)
                s = s[:mid] + " " + s[mid:]
        ofeatures.append(s)
    return ofeatures


def print_pred_wisata(y_p, item, wisata_dict, maxcount=10):
    """ print results of prediction of a new user. inputs are expected to be in
        sorted order, unscaled. """
    count = 0
    disp = [["Nama Tempat", "Kategori"]]

    for i in range(0, y_p.shape[0]):
        if count == maxcount:
            break
        count += 1
        destinasi_id = item[i, 0].astype(int)
        disp.append([wisata_dict[destinasi_id]['title'], wisata_dict[destinasi_id]['genre']])

    table = tabulate.tabulate(disp, tablefmt='html', headers="firstrow")
    return table

    return result

def gen_user_vecs(user_vec, num_items):
    """ given a user vector return:
        user predict maxtrix to match the size of item_vecs """
    user_vecs = np.tile(user_vec, (num_items, 1))
    return user_vecs

# predict on  everything, filter on print/use
def predict_uservec(user_vecs, item_vecs, model, u_s, i_s, scaler):
    """ given a scaled user vector, does the prediction on all movies in scaled print_item_vecs returns
        an array predictions sorted by predicted rating,
        arrays of user and item, sorted by predicted rating sorting index
    """
    y_p = model.predict([user_vecs[:, u_s:], item_vecs[:, i_s:]])
    y_pu = scaler.inverse_transform(y_p)

    if np.any(y_pu < 0):
        print("Error, expected all positive predictions")
    sorted_index = np.argsort(-y_pu, axis=0).reshape(-1).tolist()  #negate to get largest rating first
    sorted_ypu   = y_pu[sorted_index]
    sorted_items = item_vecs[sorted_index]
    sorted_user  = user_vecs[sorted_index]
    return(sorted_index, sorted_ypu, sorted_items, sorted_user)


# def get_item_genres(item_gvec, genre_features):
#     ''' takes in the item's genre vector and list of genre names
#     returns the feature names where gvec was 1 '''
#     offsets = np.nonzero(item_gvec)[0]
#     genres = [genre_features[i] for i in offsets]
#     return genres


def print_existing_user(y_p, y, user, items, ivs, uvs, wisata_dict, maxcount=10):
    """ print results of prediction for a user who was in the database.
        Inputs are expected to be in sorted order, unscaled.
    """
    count = 0
    disp = [["y_p", "y", "user", "user kategori ave", "destinasi rating ave", "destinasi id", "Nama tempat",]]
    count = 0
    for i in range(0, y.shape[0]):
        if y[i, 0] != 0:  # zero means not rated
            if count == maxcount:
                break
            count += 1
            destinasi_id = items[i, 0].astype(int)

            offsets = np.nonzero(items[i, ivs:] == 1)[0]
            genre_ratings = user[i, uvs + offsets]
            disp.append([y_p[i, 0], y[i, 0],
                         user[i, 0].astype(int),      # userid
                         np.array2string(genre_ratings,
                                         formatter={'float_kind':lambda x: "%.1f" % x},
                                         separator=',', suppress_small=True),
                         items[i, 2].astype(float),
                         destinasi_id,
                         wisata_dict[destinasi_id]['title'],
                         wisata_dict[destinasi_id]['genre']])

    table = tabulate.tabulate(disp, tablefmt='html', headers="firstrow", floatfmt=[".1f", ".1f", ".0f", ".2f", ".1f"])
    return table

top10destinasi_df = pd.read_csv("topten.csv")
bykategori_df = pd.read_csv("bykategori.csv")
top10destinasi_df

bykategori_df

# Load Data, set configuration variables
item_train, user_train, y_train, item_features, user_features, item_vecs, wisata_dict = load_data()

num_user_features = user_train.shape[1] - 3  # remove userid, rating count and ave rating during training
num_item_features = item_train.shape[1] - 1  # remove movie id at train time
uvs = 3  # user genre vector start
ivs = 3  # item genre vector start
u_s = 3  # start of columns to use in training, user
i_s = 1  # start of columns to use in training, items
print(f"Number of training vectors: {len(item_train)}")

pprint_train(user_train, user_features, uvs,  u_s, maxcount=5)
look = pd.read_html(pprint_train(user_train, user_features, uvs, u_s, maxcount=5))
look[0]

pprint_train(item_train, item_features, ivs, i_s, maxcount=5, user=False)
look = pd.read_html(pprint_train(item_train, item_features, ivs, i_s, maxcount=5))
look[0]

print(f"y_train[:5]: {y_train[:5]}")

# scale training data
item_train_unscaled = item_train
user_train_unscaled = user_train
y_train_unscaled    = y_train

scalerItem = StandardScaler()
scalerItem.fit(item_train)
item_train = scalerItem.transform(item_train)

scalerUser = StandardScaler()
scalerUser.fit(user_train)
user_train = scalerUser.transform(user_train)

scalerTarget = MinMaxScaler((-1, 1))
scalerTarget.fit(y_train.reshape(-1, 1))
y_train = scalerTarget.transform(y_train.reshape(-1, 1))
#ynorm_test = scalerTarget.transform(y_test.reshape(-1, 1))

print(np.allclose(item_train_unscaled, scalerItem.inverse_transform(item_train)))
print(np.allclose(user_train_unscaled, scalerUser.inverse_transform(user_train)))

item_train, item_test = train_test_split(item_train, train_size=0.80, shuffle=True, random_state=1)
user_train, user_test = train_test_split(user_train, train_size=0.80, shuffle=True, random_state=1)
y_train, y_test       = train_test_split(y_train,    train_size=0.80, shuffle=True, random_state=1)
print(f"movie/item training data shape: {item_train.shape}")
print(f"movie/item test data shape: {item_test.shape}")

pprint_train(user_train, user_features, uvs, u_s, maxcount=5)
look = pd.read_html(pprint_train(user_train, user_features, uvs, u_s, maxcount=5))
look[0]

num_outputs = 32
tf.random.set_seed(1)
user_NN = tf.keras.models.Sequential([

    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(num_outputs, activation='linear'),
])

item_NN = tf.keras.models.Sequential([

    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(num_outputs, activation='linear'),
])

input_user = tf.keras.layers.Input(shape=(num_user_features))
vu = user_NN(input_user)
vu = tf.linalg.l2_normalize(vu, axis=1)

input_item = tf.keras.layers.Input(shape=(num_item_features))
vm = item_NN(input_item)
vm = tf.linalg.l2_normalize(vm, axis=1)

output = tf.keras.layers.Dot(axes=1)([vu, vm])

model = tf.keras.Model([input_user, input_item], output)

model.summary()

tf.random.set_seed(1)
cost_fn = tf.keras.losses.MeanSquaredError()
opt = keras.optimizers.Adam(learning_rate=0.01)
model.compile(optimizer=opt,
              loss=cost_fn)

tf.random.set_seed(1)

import matplotlib.pyplot as plt

# Ambil histori pelatihan dari objek History
history = model.fit([user_train[:, u_s:], item_train[:, i_s:]], y_train, epochs=100)

# Ambil nilai loss dari setiap epoch
loss_values = history.history['loss']

# Buat grafik loss terhadap epoch
plt.plot(range(1, len(loss_values) + 1), loss_values, marker='o', linestyle='-', color='b')
plt.title('Training Loss per Epoch')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.grid(True)
plt.show()

model.evaluate([user_test[:, u_s:], item_test[:, i_s:]], y_test)

model.save("model_pariwisata.h5")

user_id = 230117
rating_count = 5
sejarahbudaya = 3
alam = 4
air = 4
kuliner = 4
religi = 2
pendidikan = 2
keluarga = 3
petualangan = 3
rating_avrg = (sejarahbudaya+alam+air+kuliner+religi+pendidikan+keluarga+petualangan)/8

user_vec = np.array([[user_id, rating_count, rating_avrg, sejarahbudaya, alam, air, kuliner, religi, pendidikan, keluarga, petualangan]])

# generate and replicate the user vector to match the number movies in the data set.
user_vecs = gen_user_vecs(user_vec,len(item_vecs))

# scale our user and item vectors
suser_vecs = scalerUser.transform(user_vecs)
sitem_vecs = scalerItem.transform(item_vecs)

# make a prediction
y_p = model.predict([suser_vecs[:, u_s:], sitem_vecs[:, i_s:]])

# unscale y prediction
y_pu = scalerTarget.inverse_transform(y_p)

# sort the results, highest prediction first
sorted_index = np.argsort(-y_pu,axis=0).reshape(-1).tolist()  #negate to get largest rating first
sorted_ypu   = y_pu[sorted_index]
sorted_items = item_vecs[sorted_index]  #using unscaled vectors for display

print_pred_wisata(sorted_ypu, sorted_items, wisata_dict, maxcount = 10)
look = pd.read_html(print_pred_wisata(sorted_ypu, sorted_items, wisata_dict, maxcount = 10))
look[0]

data = print_pred_wisata(sorted_ypu, sorted_items, wisata_dict, maxcount = 10)
print(data)

table_MN = pd.read_html(print_pred_wisata(sorted_ypu, sorted_items, wisata_dict, maxcount = 10))
df = table_MN[0]
df